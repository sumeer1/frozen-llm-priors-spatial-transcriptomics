{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a19fb6ce",
   "metadata": {},
   "source": [
    "# One-time prior synthesis (MERFISH)\n",
    "\n",
    "This notebook implements the **one-time** LLM reasoning step to synthesize gene-set priors and then **freeze** them to disk.\n",
    "1) Loads MERFISH (Squidpy)\n",
    "2) Builds a spatial kNN graph\n",
    "3) Learns spot/cell embeddings using a Graph Autoencoder (GAE)\n",
    "4) Runs one-time LLM inference to generate frozen, reusable priors:\n",
    "   - per-class gene sets from LLM justifications\n",
    "\n",
    "\n",
    "Outputs saved to `OUTDIR`:\n",
    "- `ref_llm_sets.json` — refined per-class gene sets (unordered, for audit)\n",
    "- `ref_llm_top.json`  — final frozen per-class gene sets used downstream\n",
    "- `ref_llm_w.json`    — per-class gene weights (used for weighted aggregation)\n",
    "- `marker_top.json` / `marker_w.json` — matched DE-marker baselines\n",
    "- `split.json`        — train/test indices used for (leak-free) training stats\n",
    "\n",
    "Downstream evaluation (Welch t-test + sweeps) is in Notebook 02 and **does not** require any LLM access."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846b3ec",
   "metadata": {},
   "source": [
    "## 0) Imports \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20312b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, math, time, random\n",
    "from functools import lru_cache\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from typing import List, Dict, Iterable\n",
    "\n",
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GAE, GCNConv\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63b3e2d",
   "metadata": {},
   "source": [
    "## 1)  Data loading, pre-processing and GAE model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe99c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sq.datasets.merfish()\n",
    "\n",
    "if \"Cell_class\" not in adata.obs.columns:\n",
    "    raise ValueError(\"adata.obs does not contain 'Cell_class'.\")\n",
    "\n",
    "rename_dict = {\n",
    "    \"Endothelial 1\": \"Endothelial\", \"Endothelial 2\": \"Endothelial\", \"Endothelial 3\": \"Endothelial\",\n",
    "    \"OD Mature 1\": \"OD Mature\", \"OD Mature 2\": \"OD Mature\", \"OD Mature 3\": \"OD Mature\", \"OD Mature 4\": \"OD Mature\",\n",
    "    \"OD Immature 1\": \"OD Immature\", \"OD Immature 2\": \"OD Immature\",\n",
    "}\n",
    "adata.obs[\"Cell_class\"] = adata.obs[\"Cell_class\"].replace(rename_dict)\n",
    "if \"Ambiguous\" in adata.obs[\"Cell_class\"].unique():\n",
    "    adata = adata[adata.obs[\"Cell_class\"] != \"Ambiguous\"].copy()\n",
    "\n",
    "# DE markers\n",
    "\n",
    "if \"rank_genes_groups\" not in adata.uns:\n",
    "    sc.tl.rank_genes_groups(adata, groupby=\"Cell_class\", method=\"wilcoxon\")\n",
    "\n",
    "def get_top_marker_genes_for_class(adata, cls, top_n=20):\n",
    "    rg = adata.uns.get(\"rank_genes_groups\", None)\n",
    "    if rg is None or cls not in rg[\"names\"].dtype.names:\n",
    "        return []\n",
    "    return [str(g) for g in list(rg[\"names\"][cls][:top_n])]\n",
    "\n",
    "K_MARKERS = 20\n",
    "MARKER_DICT = {c: get_top_marker_genes_for_class(adata, c, top_n=K_MARKERS) for c in ALL_CLASSES}\n",
    "\n",
    "# Build spatial kNN adjacency\n",
    "\n",
    "def build_spatial_knn_graph(adata, k=30):\n",
    "    coords = adata.obs[[\"Centroid_X\", \"Centroid_Y\"]].values\n",
    "    tree = cKDTree(coords)\n",
    "    rows, cols = [], []\n",
    "    for i, pt in enumerate(coords):\n",
    "        _, idxs = tree.query(pt, k=k + 1)\n",
    "        for nb in idxs[1:]:\n",
    "            rows.append(i); cols.append(nb)\n",
    "    mat = coo_matrix((np.ones(len(rows)), (rows, cols)), shape=(adata.n_obs, adata.n_obs))\n",
    "    return mat.maximum(mat.transpose())\n",
    "\n",
    "adj = build_spatial_knn_graph(adata, k=30)\n",
    "edge_index, _ = from_scipy_sparse_matrix(adj)\n",
    "edge_index.shape\n",
    "\n",
    "# PCA + GAE embeddings\n",
    "\n",
    "N_PCS = 30\n",
    "sc.pp.pca(adata, n_comps=N_PCS, random_state=RANDOM_SEED)\n",
    "X_pca = adata.obsm[\"X_pca\"]\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=20):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 64)\n",
    "        self.conv2 = GCNConv(64, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "model = GAE(Encoder(in_channels=N_PCS, out_channels=20)).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "data_pyg = Data(\n",
    "    x=torch.tensor(X_pca, dtype=torch.float32),\n",
    "    edge_index=edge_index\n",
    ").to(device)\n",
    "\n",
    "EPOCHS = 30\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    opt.zero_grad()\n",
    "    z = model.encode(data_pyg.x, data_pyg.edge_index)\n",
    "    loss = model.recon_loss(z, data_pyg.edge_index)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"epoch {epoch+1}/{EPOCHS} | recon_loss={loss.item():.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    SPATIAL_EMB = model.encode(data_pyg.x, data_pyg.edge_index).cpu().numpy()\n",
    "\n",
    "# top genes + neighborhood top genes\n",
    "\n",
    "VAR_NAMES = list(map(str, adata.var_names))\n",
    "\n",
    "def top_expressed_genes(adata, idx, top_n=10):\n",
    "    x = adata.X[idx]\n",
    "    x = x.A1 if hasattr(x, \"A1\") else np.asarray(x).ravel()\n",
    "    top_idx = np.argsort(x)[-top_n:][::-1]\n",
    "    return [VAR_NAMES[i] for i in top_idx]\n",
    "\n",
    "def neighbor_top_genes(adata, adj, idx, top_n=5):\n",
    "    nbrs = adj.getrow(idx).indices\n",
    "    if len(nbrs) == 0:\n",
    "        return []\n",
    "    avg = np.array(adata.X[nbrs].mean(axis=0)).ravel()\n",
    "    top_idx = np.argsort(avg)[-top_n:][::-1]\n",
    "    return [VAR_NAMES[i] for i in top_idx]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af58e5b",
   "metadata": {},
   "source": [
    "## 2) Vertex AI / LLM set up \n",
    "\n",
    "If we need to run LLM calls, uncomment the imports and use the agent helpers below.\n",
    "We can skip LLM calls entirely, if we have already have `llm_records` saved from a prior run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a01db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vertex environment (set these in your shell or here)\n",
    "GOOGLE_CLOUD_PROJECT  = os.getenv(\"GOOGLE_CLOUD_PROJECT\",  \"YOUR_PROJECT_ID\")\n",
    "GOOGLE_CLOUD_LOCATION = os.getenv(\"GOOGLE_CLOUD_LOCATION\", \"global\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49ae22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_CLASSES = sorted(map(str, adata.obs[\"Cell_class\"].unique()))\n",
    "\n",
    "def build_prompt(cell_idx: int, candidate_classes: List[str], k_markers_per_class: int = 10) -> str:\n",
    "    marker_lines = []\n",
    "    for c in candidate_classes:\n",
    "        mk = _hk_filter_to_panel(MARKER_DICT.get(c, [])[:k_markers_per_class], panel)\n",
    "        marker_lines.append(f\"- {c}: {', '.join(mk) if mk else 'none'}\")\n",
    "    \n",
    "    expr = top_expressed_genes(adata, cell_idx)\n",
    "    nbr  = neighbor_top_genes(adata, adj, cell_idx)\n",
    "    emb  = SPATIAL_EMB[cell_idx][:5]\n",
    "    norm = float(np.linalg.norm(SPATIAL_EMB[cell_idx]))\n",
    "    return (\n",
    "        \"We are classifying a MERFISH cell using marker genes, expression, neighbour context, and spatial embedding.\\n\"\n",
    "        f\"Marker genes (hints): {', '.join(marker_lines) or 'none'}.\\n\"\n",
    "        f\"Top expressed genes: {', '.join(expr)}.\\n\"\n",
    "        f\"Neighbour genes: {', '.join(nbr) or 'none'}.\\n\"\n",
    "        f\"Embedding dims: [{', '.join(f'{v:.2f}' for v in emb)}], norm ~ {norm:.2f}.\\n\"\n",
    "        f\"Candidate classes: {candidate_classes}.\\n\"\n",
    "        \"Return *only* JSON with keys 'label','confidence','justification'.\\n\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8306437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_agents():\n",
    "    tools = []\n",
    "    inf = Agent(\n",
    "        role=\"Bioinformatics Researcher\",\n",
    "        goal=\"Predict MERFISH cell type and return JSON {label, confidence, justification}.\",\n",
    "        backstory=\"You classify spatial single cells using gene markers and neighborhood context.\",\n",
    "        allow_delegation=False,\n",
    "        tools=tools,\n",
    "        llm=LLM(\n",
    "            model=\"vertex_ai/gemini-2.5-pro\",\n",
    "            temperature=0.7,\n",
    "            vertex_project=os.environ[\"GOOGLE_CLOUD_PROJECT\"],\n",
    "            vertex_location=os.environ[\"GOOGLE_CLOUD_LOCATION\"],\n",
    "        ),\n",
    "        verbose=False,\n",
    "        memory=False,\n",
    "    )\n",
    "    rev = Agent(\n",
    "        role=\"Senior QA Bioinformatician\",\n",
    "        goal=\"Review predictions and return JSON {'label': <class>} only.\",\n",
    "        backstory=\"Senior scientist checks markers and adjusts labels.\",\n",
    "        allow_delegation=True,\n",
    "        tools=tools,\n",
    "        llm=LLM(\n",
    "            model=\"vertex_ai/gemini-2.5-pro\",\n",
    "            temperature=0.7,\n",
    "            vertex_project=os.environ[\"GOOGLE_CLOUD_PROJECT\"],\n",
    "            vertex_location=os.environ[\"GOOGLE_CLOUD_LOCATION\"],\n",
    "        ),\n",
    "        verbose=False,\n",
    "        memory=False,\n",
    "    )\n",
    "    return inf, rev\n",
    "\n",
    "# inf, rev = configure_agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ed37bb",
   "metadata": {},
   "source": [
    "## 3) Bootstrap indices + `llm_records`\n",
    "\n",
    "`llm_records` must be a list of dicts with keys at least: `idx`, `label`, `confidence`, `justification`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c0993e",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = \"./\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "LLM_RECORDS_PATH = os.path.join(OUTDIR, \"llm_records.json\")\n",
    "\n",
    "\n",
    "\n",
    "def balanced_indices(n_per_class: int = 20, seed: int = 42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    y = adata.obs[\"Cell_class\"].astype(str).values\n",
    "    out = []\n",
    "    for ct in ALL_CLASSES:\n",
    "        idxs = np.where(y == ct)[0]\n",
    "        take = min(len(idxs), n_per_class)\n",
    "        if take > 0:\n",
    "            out.extend(rng.choice(idxs, take, replace=False))\n",
    "    return np.array(out, dtype=int)\n",
    "\n",
    "boot_idx = balanced_indices(n_per_class=20, seed=42)\n",
    "print(\"Boot cells:\", len(boot_idx))\n",
    "\n",
    "\n",
    "# Option A: generate llm_records by calling the LLM (run once) \n",
    "def call_llm_once(prompt: str, agent) -> Dict:\n",
    "    task = Task(description=prompt, expected_output=\"JSON with keys 'label','confidence','justification'\", agent=agent)\n",
    "    result = Crew(agents=[agent], tasks=[task], process=Process.sequential).kickoff()\n",
    "    raw = getattr(result, \"raw\", None)\n",
    "    if not raw or not isinstance(raw, str) or not raw.strip():\n",
    "        raise ValueError(\"Invalid response from LLM call.\")\n",
    "    txt = re.sub(r\"```json|```\", \"\", raw).strip()\n",
    "    js = json.loads(txt)\n",
    "    if isinstance(js, list) and js and isinstance(js[0], dict):\n",
    "        js = js[0]\n",
    "    if not isinstance(js, dict) or \"label\" not in js:\n",
    "        raise ValueError(f\"LLM JSON missing 'label': {js}\")\n",
    "    return js\n",
    "\n",
    "for i in boot_idx:\n",
    "    prompt = build_prompt(int(i), ALL_CLASSES, k_markers_per_class=10)\n",
    "    js = call_llm_once(prompt, inf_agent)\n",
    "    llm_records.append({\"idx\": int(i), **js})\n",
    "\n",
    "with open(LLM_RECORDS_PATH, \"w\") as f:\n",
    "    json.dump(llm_records, f, indent=2)\n",
    "print(f\"Saved llm_records → {LLM_RECORDS_PATH}\")\n",
    "\n",
    "\n",
    "# Option B: load existing records  \n",
    "if os.path.exists(LLM_RECORDS_PATH):\n",
    "    with open(LLM_RECORDS_PATH) as f:\n",
    "        llm_records = json.load(f)\n",
    "    print(f\"Loaded llm_records: {len(llm_records)} from {LLM_RECORDS_PATH}\")\n",
    "else:\n",
    "    llm_records = []\n",
    "    print(\"No llm_records.json found. To generate it, enable Option A below.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18490aa2",
   "metadata": {},
   "source": [
    "## 4) Build raw per-class LLM gene sets from justifications\n",
    "\n",
    "We extract gene mentions from the LLM `justification` field, filter to the gene panel, and drop housekeeping genes.  \n",
    "We then **bucket** these genes by the **true class** of each bootstrap cell (`use_true_class=True`) since the bootstrap set is labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb98bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOUSEKEEPING_RE = r'^(Rpl|Rps|Mrpl|Mrps|mt\\-|Mt\\-)'\n",
    "panel = set(map(str, adata.var_names))\n",
    "\n",
    "def _hk_filter_to_panel(gs, panel):\n",
    "    gs = [str(g) for g in gs if str(g) in panel]\n",
    "    gs = [g for g in gs if not re.match(HOUSEKEEPING_RE, g)]\n",
    "    out, seen = [], set()\n",
    "    for g in gs:\n",
    "        if g not in seen:\n",
    "            seen.add(g); out.append(g)\n",
    "    return out\n",
    "\n",
    "\n",
    "def extract_gene_mentions(text: str) -> List[str]:\n",
    "    \"\"\"Simple gene-symbol extractor: keep tokens that match adata.var_names (case-insensitive).\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    # map uppercase -> canonical panel symbol\n",
    "    up = {str(g).upper(): str(g) for g in map(str, adata.var_names)}\n",
    "    toks = re.findall(r\"\\b[A-Za-z0-9\\-]{2,}\\b\", text)\n",
    "    out = []\n",
    "    for t in toks:\n",
    "        if t.upper() in up:\n",
    "            out.append(up[t.upper()])\n",
    "    # unique keep order\n",
    "    seen=set(); uniq=[]\n",
    "    for g in out:\n",
    "        if g not in seen:\n",
    "            seen.add(g); uniq.append(g)\n",
    "    return uniq\n",
    "\n",
    "def llm_gene_sets_from_records_frozen(\n",
    "    llm_records: List[dict],\n",
    "    use_true_class: bool = True,\n",
    "    min_genes: int = 3,\n",
    "    topup_from_markers: bool = True,\n",
    ") -> Dict[str, set]:\n",
    "    class2genes = {ct: set() for ct in ALL_CLASSES}\n",
    "    for r in llm_records:\n",
    "        genes = extract_gene_mentions(r.get(\"justification\",\"\"))\n",
    "        genes = _hk_filter_to_panel(genes, panel)\n",
    "        if not genes:\n",
    "            continue\n",
    "        ct = (str(adata.obs[\"Cell_class\"].iloc[int(r[\"idx\"])])\n",
    "              if use_true_class else str(r.get(\"label\",\"None\")))\n",
    "        if ct in class2genes:\n",
    "            class2genes[ct].update(genes)\n",
    "\n",
    "    if topup_from_markers:\n",
    "        for ct, gs in class2genes.items():\n",
    "            if len(gs) < min_genes:\n",
    "                extras = _hk_filter_to_panel(MARKER_DICT.get(ct, []), panel)\n",
    "                need = max(0, min_genes - len(gs))\n",
    "                class2genes[ct] = set(list(gs) + extras[:need])\n",
    "    return class2genes\n",
    "\n",
    "llm_sets_raw = llm_gene_sets_from_records_frozen(\n",
    "    llm_records,\n",
    "    use_true_class=True,\n",
    "    min_genes=3,\n",
    "    topup_from_markers=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbaad28",
   "metadata": {},
   "source": [
    "## 5) Refine gene sets on TRAIN only  and compute weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a65cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_GENES = 3\n",
    "\n",
    "def _cohens_d_1gene(pos, neg):\n",
    "    pos = np.asarray(pos, float); neg = np.asarray(neg, float)\n",
    "    if len(pos) < 2 or len(neg) < 2:\n",
    "        return np.nan\n",
    "    mx, my = pos.mean(), neg.mean()\n",
    "    vx, vy = pos.var(ddof=1), neg.var(ddof=1)\n",
    "    denom = (len(pos) + len(neg) - 2)\n",
    "    sp2 = ((len(pos)-1)*vx + (len(neg)-1)*vy) / denom if denom > 0 else np.nan\n",
    "    if not np.isfinite(sp2) or sp2 <= 0:\n",
    "        return np.nan\n",
    "    return (mx - my) / math.sqrt(sp2)\n",
    "\n",
    "def _per_gene_stats(X, y, gene_names, cls, genes, train_idx):\n",
    "    g2i = {g:i for i,g in enumerate(gene_names)}\n",
    "    idx_pos = [i for i in train_idx if y[i] == cls]\n",
    "    idx_neg = [i for i in train_idx if y[i] != cls]\n",
    "    out = {}\n",
    "    for g in genes:\n",
    "        if g not in g2i:\n",
    "            continue\n",
    "        gi = g2i[g]\n",
    "        pos = X[idx_pos, gi]; neg = X[idx_neg, gi]\n",
    "        diff = float(np.mean(pos) - np.mean(neg)) if len(pos) and len(neg) else np.nan\n",
    "        d = _cohens_d_1gene(pos, neg)\n",
    "        out[g] = (diff, d)\n",
    "    return out\n",
    "\n",
    "def build_refined_llm_sets(adata, llm_sets_raw, marker_dict, train_idx,\n",
    "                           target_N=18, min_gene_d=0.10, keep_if_pos_only=True):\n",
    "    X = adata.X.toarray() if hasattr(adata.X, \"toarray\") else np.asarray(adata.X)\n",
    "    y = adata.obs[\"Cell_class\"].astype(str).values\n",
    "    gene_names = np.array(adata.var_names, dtype=str)\n",
    "    panel = set(map(str, gene_names))\n",
    "\n",
    "    llm_f = {c: _hk_filter_to_panel(list(gs), panel) for c, gs in llm_sets_raw.items()}\n",
    "\n",
    "    refined_sets, gene_weights = {}, {}\n",
    "    for c, llm_list in llm_f.items():\n",
    "        if len(llm_list) == 0:\n",
    "            continue\n",
    "\n",
    "        stats = _per_gene_stats(X, y, gene_names, c, llm_list, train_idx)\n",
    "\n",
    "        keep, wts = [], {}\n",
    "        for gene in llm_list:\n",
    "            diff, d = stats.get(gene, (np.nan, np.nan))\n",
    "            if not np.isfinite(diff):\n",
    "                continue\n",
    "            if keep_if_pos_only and diff <= 0:\n",
    "                continue\n",
    "            if np.isfinite(d) and d < min_gene_d:\n",
    "                continue\n",
    "            keep.append(gene)\n",
    "            wts[gene] = max(float(d), 0.0) if np.isfinite(d) else 0.0\n",
    "\n",
    "        if len(keep) < MIN_GENES:\n",
    "            scored = [(stats.get(g,(0,0))[0], stats.get(g,(0,0))[1], g) for g in llm_list]\n",
    "            scored.sort(key=lambda t: (t[0], t[1]), reverse=True)\n",
    "            keep = [g for _,__,g in scored[:max(MIN_GENES, min(target_N, len(scored)))]]\n",
    "            wts  = {g: max(stats.get(g,(0,0))[1], 0.0) for g in keep}\n",
    "\n",
    "        rk = _hk_filter_to_panel(marker_dict.get(c, []), panel)\n",
    "        for mg in rk:\n",
    "            if len(keep) >= target_N:\n",
    "                break\n",
    "            if mg not in keep:\n",
    "                keep.append(mg)\n",
    "                wts[mg] = wts.get(mg, 0.05)\n",
    "\n",
    "        vals = np.array([wts[g] for g in keep], float)\n",
    "        if np.isfinite(vals).all() and vals.sum() > 0:\n",
    "            scale = float(len(vals) / vals.sum())\n",
    "            for k in keep:\n",
    "                wts[k] *= scale\n",
    "        else:\n",
    "            for k in keep:\n",
    "                wts[k] = 1.0\n",
    "\n",
    "        refined_sets[c] = set(keep[:target_N])\n",
    "        gene_weights[c] = {g: float(wts[g]) for g in refined_sets[c]}\n",
    "    return refined_sets, gene_weights\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "train_idx, test_idx = next(skf.split(adata.X, adata.obs[\"Cell_class\"]))\n",
    "\n",
    "\n",
    "marker_ranked = {c: MARKER_DICT.get(c, [])[:200] for c in ALL_CLASSES}\n",
    "\n",
    "ref_llm_sets, ref_llm_w = build_refined_llm_sets(\n",
    "    adata=adata,\n",
    "    llm_sets_raw=llm_sets_raw,\n",
    "    marker_dict=marker_ranked,\n",
    "    train_idx=train_idx,\n",
    "    target_N=18,\n",
    "    min_gene_d=0.10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18a1863",
   "metadata": {},
   "source": [
    "## 6) Freeze final priors and build a matched marker baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1e3d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = \"./\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "def sets_to_jsonable(d):\n",
    "    return {str(k): sorted(map(str, v)) for k, v in d.items()}\n",
    "\n",
    "def weights_to_jsonable(d):\n",
    "    return {str(c): {str(g): float(w) for g, w in gm.items()} for c, gm in d.items()}\n",
    "\n",
    "def freeze_llm_top(ref_llm_sets, ref_llm_w):\n",
    "    out = {}\n",
    "    for c in ref_llm_sets:\n",
    "        llm_sorted = sorted(ref_llm_sets[c], key=lambda g: ref_llm_w.get(c, {}).get(g, 0.0), reverse=True)\n",
    "        out[c] = set(llm_sorted)\n",
    "    return out\n",
    "\n",
    "def make_marker_baseline(ref_llm_top, marker_ranked):\n",
    "    marker_top, marker_w = {}, {}\n",
    "    for c in ref_llm_top:\n",
    "        n_match = len(ref_llm_top[c])\n",
    "        m_full = _hk_filter_to_panel(marker_ranked.get(c, []), panel)\n",
    "        m_take = m_full[:n_match]\n",
    "        marker_top[c] = set(m_take)\n",
    "        marker_w[c] = {g: 1.0/(i+1) for i, g in enumerate(m_take)}\n",
    "    return marker_top, marker_w\n",
    "\n",
    "ref_llm_top = freeze_llm_top(ref_llm_sets, ref_llm_w)\n",
    "marker_top, marker_w = make_marker_baseline(ref_llm_top, marker_ranked)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d111c2b7",
   "metadata": {},
   "source": [
    "## 7) Save priors for (Welch t-test evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0a919",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sets_to_jsonable(d):\n",
    "    return {str(k): sorted(map(str, v)) for k, v in d.items()}\n",
    "\n",
    "def weights_to_jsonable(d):\n",
    "    return {str(c): {str(g): float(w) for g, w in gm.items()} for c, gm in d.items()}\n",
    "\n",
    "with open(os.path.join(OUTDIR, \"llm_records.json\"), \"w\") as f:\n",
    "    json.dump(llm_records, f, indent=2)\n",
    "\n",
    "with open(os.path.join(OUTDIR, \"ref_llm_sets.json\"), \"w\") as f:\n",
    "    json.dump(sets_to_jsonable(ref_llm_sets), f, indent=2)\n",
    "with open(os.path.join(OUTDIR, \"ref_llm_top.json\"), \"w\") as f:\n",
    "    json.dump(sets_to_jsonable(ref_llm_top), f, indent=2)\n",
    "with open(os.path.join(OUTDIR, \"ref_llm_w.json\"), \"w\") as f:\n",
    "    json.dump(weights_to_jsonable(ref_llm_w), f, indent=2)\n",
    "\n",
    "with open(os.path.join(OUTDIR, \"marker_top.json\"), \"w\") as f:\n",
    "    json.dump(sets_to_jsonable(marker_top), f, indent=2)\n",
    "with open(os.path.join(OUTDIR, \"marker_w.json\"), \"w\") as f:\n",
    "    json.dump(weights_to_jsonable(marker_w), f, indent=2)\n",
    "\n",
    "with open(os.path.join(OUTDIR, \"split.json\"), \"w\") as f:\n",
    "    json.dump({\"train_idx\": list(map(int, train_idx)),\n",
    "               \"test_idx\":  list(map(int, test_idx))}, f, indent=2)\n",
    "\n",
    "print(\"Saved frozen priors to:\", OUTDIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Flows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
