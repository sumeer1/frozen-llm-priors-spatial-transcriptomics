{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999cdc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from functools import lru_cache\n",
    "\n",
    "import scanpy as sc\n",
    "import squidpy as sq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial import cKDTree\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import GCNConv, GAE\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import from_scipy_sparse_matrix\n",
    "\n",
    "from crewai import Agent, LLM, Task, Crew, Process\n",
    "\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Config\n",
    "# -----------------------------------------------------------------------------\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"ringed-codex-468710-j7\"\n",
    "os.environ[\"GOOGLE_CLOUD_LOCATION\"] = \"global\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Load spatial data\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Loading IPF ST dataset...\")\n",
    "file_path = '/../my_new_data.h5ad'\n",
    "adata = sc.read_h5ad(file_path)\n",
    "print(adata.obs[\"cell.type\"].value_counts())\n",
    "\n",
    "if \"cell.type\" not in adata.obs:\n",
    "    raise ValueError(\"Dataset must have 'cell.type' column.\")\n",
    "\n",
    "# For consistency in code, create a 'Cell_class' column copying 'cell.type' (if needed)\n",
    "adata.obs['Cell_class'] = adata.obs['cell.type']\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Ligand–Receptor database\n",
    "# -----------------------------------------------------------------------------\n",
    "logger.info(\"Loading ligand–receptor reference...\")\n",
    "lr_pairs = pd.read_csv(\"/../combined_ligand_receptor_pairs.csv\")  # supply your own ligand-receptor file\n",
    "\n",
    "genes_in_data = set(adata.var_names)\n",
    "lr_pairs = lr_pairs[\n",
    "    lr_pairs[\"ligand_gene\"].isin(genes_in_data) &\n",
    "    lr_pairs[\"receptor_gene\"].isin(genes_in_data)\n",
    "].reset_index(drop=True)\n",
    "logger.info(f\"Filtered to {len(lr_pairs)} ligand–receptor pairs in dataset.\")\n",
    "\n",
    "# Build a dictionary ligand -> list of receptors (for faster lookup)\n",
    "ligand_receptor_dict = {}\n",
    "for _, row in lr_pairs.iterrows():\n",
    "    ligand = row['ligand_gene']\n",
    "    receptor = row['receptor_gene']\n",
    "    if ligand not in ligand_receptor_dict:\n",
    "        ligand_receptor_dict[ligand] = []\n",
    "    ligand_receptor_dict[ligand].append(receptor)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Spatial graph & GAE embedding\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_spatial_knn_graph(adata, k=10):\n",
    "    coords = adata.obs[[\"original_x\", \"original_y\"]].values\n",
    "    tree = cKDTree(coords)\n",
    "    rows, cols = [], []\n",
    "    for i, pt in enumerate(coords):\n",
    "        _, idxs = tree.query(pt, k=k+1)\n",
    "        for nb in idxs[1:]:\n",
    "            rows.append(i)\n",
    "            cols.append(nb)\n",
    "    mat = coo_matrix((np.ones(len(rows)), (rows, cols)), shape=(adata.n_obs, adata.n_obs))\n",
    "    return mat.maximum(mat.transpose())\n",
    "\n",
    "logger.info(\"Building spatial KNN graph...\")\n",
    "knn_adjacency = build_spatial_knn_graph(adata, k=10)\n",
    "\n",
    "logger.info(\"Running PCA...\")\n",
    "n_pcs = 30\n",
    "sc.pp.pca(adata, n_comps=n_pcs, random_state=RANDOM_SEED)\n",
    "X_pca = adata.obsm[\"X_pca\"]\n",
    "\n",
    "logger.info(\"Preparing data for GAE...\")\n",
    "edge_index, _ = from_scipy_sparse_matrix(knn_adjacency)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_pyg = Data(x=torch.tensor(X_pca, dtype=torch.float32), edge_index=edge_index).to(device)\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 64)\n",
    "        self.conv2 = GCNConv(64, out_channels)\n",
    "    def forward(self, x, edge_index):\n",
    "        x = torch.relu(self.conv1(x, edge_index))\n",
    "        return self.conv2(x, edge_index)\n",
    "\n",
    "model = GAE(Encoder(in_channels=n_pcs, out_channels=20)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "logger.info(\"Training GAE...\")\n",
    "model.train()\n",
    "for epoch in range(30):\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(data_pyg.x, data_pyg.edge_index)\n",
    "    loss = model.recon_loss(z, data_pyg.edge_index)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    logger.info(f\"Epoch {epoch+1}/30, Loss={loss.item():.4f}\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    spatial_embeddings = model.encode(data_pyg.x, data_pyg.edge_index).cpu().numpy()\n",
    "logger.info(\"Computed spatial embeddings.\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Interaction extraction functions\n",
    "# -----------------------------------------------------------------------------\n",
    "def get_cell_neighbors_types_and_expr(adata, knn_adj, idx):\n",
    "    nbr_idxs = knn_adj.getrow(idx).indices\n",
    "    nbr_types = adata.obs['Cell_class'].iloc[nbr_idxs].values\n",
    "    nbr_expr = adata.X[nbr_idxs]\n",
    "    return nbr_idxs, nbr_types, nbr_expr\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "def get_ligand_receptor_candidates_with_norms_and_proximity(adata, knn_adj, cell_idx, spatial_embeddings, expr_threshold=1):\n",
    "    cell_type = adata.obs['Cell_class'].iloc[cell_idx]\n",
    "    nbr_idxs = knn_adj.getrow(cell_idx).indices\n",
    "    nbr_types = adata.obs['Cell_class'].iloc[nbr_idxs].values\n",
    "    #\n",
    "    # Source expression\n",
    "    cell_expr = adata.X[cell_idx]\n",
    "    if hasattr(cell_expr, \"toarray\"):\n",
    "        cell_expr = cell_expr.toarray().flatten()\n",
    "    elif hasattr(cell_expr, \"A1\"):\n",
    "        cell_expr = cell_expr.A1\n",
    "    else:\n",
    "        cell_expr = np.asarray(cell_expr).flatten()\n",
    "    #\n",
    "    cell_genes = adata.var_names[np.where(cell_expr > expr_threshold)[0]]\n",
    "    #\n",
    "    interactions = []\n",
    "    source_vec = spatial_embeddings[cell_idx]  # <-- define once\n",
    "    source_norm = np.linalg.norm(source_vec)\n",
    "    #\n",
    "    for local_i, nbr_i in enumerate(nbr_idxs):\n",
    "        nbr_expr_vec = adata.X[nbr_i]\n",
    "        if hasattr(nbr_expr_vec, \"toarray\"):\n",
    "            nbr_expr_vec = nbr_expr_vec.toarray().flatten()\n",
    "        elif hasattr(nbr_expr_vec, \"A1\"):\n",
    "            nbr_expr_vec = nbr_expr_vec.A1\n",
    "        else:\n",
    "            nbr_expr_vec = np.asarray(nbr_expr_vec).flatten()\n",
    "            #\n",
    "        nbr_genes = adata.var_names[nbr_expr_vec > expr_threshold]\n",
    "        target_type = nbr_types[local_i]\n",
    "        target_vec = spatial_embeddings[nbr_i]\n",
    "        target_norm = np.linalg.norm(target_vec)\n",
    "        #\n",
    "        # Proximity metrics\n",
    "        cosine_sim = cosine_similarity(\n",
    "            source_vec.reshape(1, -1),\n",
    "            target_vec.reshape(1, -1)\n",
    "        )[0, 0]\n",
    "        euclidean_dist = np.linalg.norm(source_vec - target_vec)\n",
    "        #\n",
    "        for lig in cell_genes:\n",
    "            if lig in ligand_receptor_dict:\n",
    "                for rec in ligand_receptor_dict[lig]:\n",
    "                    if rec in nbr_genes:\n",
    "                        interactions.append({\n",
    "                            'source_cell': cell_idx,\n",
    "                            'source_type': cell_type,\n",
    "                            'target_cell': nbr_i,\n",
    "                            'target_type': target_type,\n",
    "                            'ligand': lig,\n",
    "                            'receptor': rec,\n",
    "                            'source_norm': source_norm,\n",
    "                            'target_norm': target_norm,\n",
    "                            'cosine_similarity': cosine_sim,\n",
    "                            'euclidean_distance': euclidean_dist,\n",
    "                        })\n",
    "    return interactions\n",
    "\n",
    "def stratified_sample_cells(adata, n_cells_total):\n",
    "    # Get unique cell types\n",
    "    cell_types = adata.obs['Cell_class'].unique()\n",
    "    n_types = len(cell_types)\n",
    "    \n",
    "    # How many samples per cell type (integer division)\n",
    "    n_per_type = max(1, n_cells_total // n_types)\n",
    "    \n",
    "    sampled_idxs = []\n",
    "    for ctype in cell_types:\n",
    "        # Indices for this cell type\n",
    "        idxs = np.where(adata.obs['Cell_class'] == ctype)[0]\n",
    "        # If not enough cells, sample all; else random sample n_per_type without replacement\n",
    "        if len(idxs) <= n_per_type:\n",
    "            sampled_idxs.extend(idxs.tolist())\n",
    "        else:\n",
    "            sampled_idxs.extend(np.random.choice(idxs, size=n_per_type, replace=False).tolist())\n",
    "    \n",
    "    return np.array(sampled_idxs)\n",
    "\n",
    "def aggregate_interactions_with_norms(adata, knn_adj, spatial_embeddings, n_cells=100, expr_threshold=1):\n",
    "    all_interactions = []\n",
    "    #sampled_idxs = np.random.choice(adata.n_obs, size=n_cells, replace=False)\n",
    "    sampled_idxs = stratified_sample_cells(adata, n_cells)\n",
    "    for i in sampled_idxs:\n",
    "        all_interactions.extend(get_ligand_receptor_candidates_with_norms_and_proximity(adata, knn_adj, i, spatial_embeddings, expr_threshold=expr_threshold))\n",
    "    df_int = pd.DataFrame(all_interactions)\n",
    "    #\n",
    "    # Convert categorical columns to string to avoid grouping errors\n",
    "    for col in ['source_type', 'target_type', 'ligand', 'receptor']:\n",
    "        df_int[col] = df_int[col].astype(str)\n",
    "    #\n",
    "    # Aggregate count and average norms by source_type, target_type, ligand, receptor\n",
    "    agg = df_int.groupby(['source_type', 'target_type', 'ligand', 'receptor']).agg(\n",
    "        count=('ligand', 'size'),\n",
    "        avg_source_norm=('source_norm', 'mean'),\n",
    "        avg_target_norm=('target_norm', 'mean'),\n",
    "        avg_cosine_similarity=('cosine_similarity', 'mean'),\n",
    "    ).reset_index()\n",
    "    #\n",
    "    return agg\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Prompt builder using norm of embeddings\n",
    "# -----------------------------------------------------------------------------\n",
    "def build_cci_prompt_with_norms(row, marker_dict):\n",
    "    source_type = row['source_type']\n",
    "    target_type = row['target_type']\n",
    "    ligand = row['ligand']\n",
    "    receptor = row['receptor']\n",
    "    count = row['count']\n",
    "    avg_source_norm = row['avg_source_norm']\n",
    "    avg_target_norm = row['avg_target_norm']\n",
    "    avg_cosine_similarity = row['avg_cosine_similarity']\n",
    "    #\n",
    "    source_markers = marker_dict.get(source_type, [])\n",
    "    target_markers = marker_dict.get(target_type, [])\n",
    "    #\n",
    "    return (\n",
    "        f\"Analyze the following potential cell–cell interaction in the Idiopathic Pulmonary Fibrosis (IPF) dataset.\\n\\n\"\n",
    "        \n",
    "        f\"--- CELL TYPE CONTEXT ---\\n\"\n",
    "        f\"Source cell type: {source_type}\\n\"\n",
    "        f\"  Representative marker genes (for identifying cell type only): {', '.join(source_markers)}\\n\"\n",
    "        f\"Target cell type: {target_type}\\n\"\n",
    "        f\"  Representative marker genes (for identifying cell type only): {', '.join(target_markers)}\\n\\n\"\n",
    "        \n",
    "        f\"--- LIGAND–RECEPTOR CANDIDATE ---\\n\"\n",
    "        f\"Ligand: {ligand} (expressed above threshold in the source cell)\\n\"\n",
    "        f\"Receptor: {receptor} (expressed above threshold in the target cell)\\n\"\n",
    "        f\"Expression threshold applied per cell, not by average per cell type.\\n\\n\"\n",
    "        \n",
    "        f\"--- SPATIAL OCCURRENCE ---\\n\"\n",
    "        f\"Observed in {count} distinct spatial neighborhoods (frequency of co-occurrence of ligand and receptor expression in nearby cells).\\n\"\n",
    "        f\"Average source cell embedding norm: {avg_source_norm:.2f} (reflecting position strength in learned spatial–expression space)\\n\"\n",
    "        f\"Average target cell embedding norm: {avg_target_norm:.2f} (same scale as above)\\n\"\n",
    "        f\"Higher norms may correspond to stronger local spatial signal.\\n\\n\"\n",
    "        f\"Average cosine similarity: {avg_cosine_similarity:.3f} (1 = identical spatial context, 0 = unrelated)\\n\"\n",
    "        f\"Cosine similarity is the primary indicators of spatial proximity.\\n\\n\"\n",
    "\n",
    "        f\"--- TASK ---\\n\"\n",
    "        f\"Based on the cell type context, ligand–receptor expression evidence, and spatial occurrence data, \"\n",
    "        f\"decide whether this represents a plausible biological interaction in the IPF context. \"\n",
    "        f\"Return a JSON object with the following keys:\\n\"\n",
    "        f\"- 'interaction'\\n\"\n",
    "        f\"- 'confidence': number between 0 and 1\\n\"\n",
    "        f\"- 'justification': short explanation grounded in the provided data and known biology.\"\n",
    "    )\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Agent setup functions\n",
    "# -----------------------------------------------------------------------------\n",
    "def configure_agents():\n",
    "    tools = []\n",
    "    inf = Agent(\n",
    "        role=\"Bioinformatics Researcher\",\n",
    "        goal=\"Analyze potential ligand-receptor interactions between cell types in Idiopathic Pulmonary Fibrosis (IPF) data and return JSON with keys 'interaction', 'confidence', 'justification'.\",\n",
    "        backstory=(\n",
    "            \"A junior computational biologist focused on cell-cell communication inference from spatial transcriptomics data, \"\n",
    "            \"using marker genes, ligand-receptor databases, spatial neighborhood info, and embeddings.\"\n",
    "        ),\n",
    "        allow_delegation=False,\n",
    "        tools=tools,\n",
    "        llm=LLM(\n",
    "            model=\"vertex_ai/gemini-2.5-flash\",\n",
    "            temperature=0.7,\n",
    "            vertex_project=os.environ[\"GOOGLE_CLOUD_PROJECT\"],\n",
    "            vertex_location=os.environ[\"GOOGLE_CLOUD_LOCATION\"],\n",
    "        ),\n",
    "        verbose=False,\n",
    "        memory=False\n",
    "    )\n",
    "    rev = Agent(\n",
    "        role=\"Senior Quality Assurance Bioinformatician\",\n",
    "        goal=\"Review a junior agent’s ligand-receptor interaction prediction and return JSON with a single key 'interaction'.\",\n",
    "        backstory=(\n",
    "            \"A senior scientist specializing in cell-cell interaction validation in spatial omics.\"\n",
    "        ),\n",
    "        allow_delegation=False,\n",
    "        tools=tools,\n",
    "        llm=LLM(\n",
    "            model=\"vertex_ai/gemini-2.5-flash\",\n",
    "            temperature=0.7,\n",
    "            vertex_project=os.environ[\"GOOGLE_CLOUD_PROJECT\"],\n",
    "            vertex_location=os.environ[\"GOOGLE_CLOUD_LOCATION\"],\n",
    "        ),\n",
    "        verbose=False,\n",
    "        memory=False\n",
    "    )\n",
    "    return inf, rev\n",
    "\n",
    "inference_agent, review_agent = configure_agents()\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def call_inference(prompt):\n",
    "    for attempt in range(3):\n",
    "        start = time.time()\n",
    "        task = Task(description=prompt, expected_output=\"JSON with keys 'interaction','confidence','justification'\", output_file=\"inf_cci.json\", agent=inference_agent)\n",
    "        try:\n",
    "            raw = Crew(agents=[inference_agent], tasks=[task], process=Process.sequential).kickoff().raw\n",
    "            logger.info(f\"Inference call done in {time.time()-start:.2f}s\")\n",
    "            js = json.loads(re.sub(r\"```json|```\", \"\", raw).strip())\n",
    "            if isinstance(js, list) and len(js) > 0 and isinstance(js[0], dict):\n",
    "                js = js[0]\n",
    "            return js\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Inference attempt {attempt+1} failed: {e}\")\n",
    "            time.sleep(1)\n",
    "    return {\"interaction\": \"None\", \"confidence\": 0.0, \"justification\": \"\"}\n",
    "\n",
    "def call_review(inf_js):\n",
    "    prompt = f\"Candidates: potential interactions\\nInference JSON: {json.dumps(inf_js)}\\nReturn JSON with single key 'interaction'.\"\n",
    "    start = time.time()\n",
    "    task = Task(description=prompt, expected_output=\"JSON with key 'interaction'\", agent=review_agent)\n",
    "    raw = Crew(agents=[review_agent], tasks=[task], process=Process.sequential).kickoff().raw\n",
    "    logger.info(f\"Review call done in {time.time()-start:.2f}s\")\n",
    "    try:\n",
    "        rev = json.loads(re.sub(r\"```json|```\", \"\", raw).strip())\n",
    "        inter = rev.get('interaction', 'None')\n",
    "        return inter\n",
    "    except:\n",
    "        return 'None'\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Evaluation pipeline for cell-cell interactions\n",
    "# -----------------------------------------------------------------------------\n",
    "def evaluate_cci_pipeline_with_norms(n_samples=100, use_review=False, max_workers=8):\n",
    "    logger.info(f\"Aggregating ligand-receptor interactions on {n_samples} cells (with norms)...\")\n",
    "    agg_df = aggregate_interactions_with_norms(adata, knn_adjacency, spatial_embeddings, n_cells=n_samples)\n",
    "    #\n",
    "    records = []\n",
    "    logger.info(f\"Running inference on {len(agg_df)} candidate interactions...\")\n",
    "    #\n",
    "    from concurrent.futures import ThreadPoolExecutor\n",
    "    #\n",
    "    def worker(row):\n",
    "        prompt = build_cci_prompt_with_norms(row, marker_dict)\n",
    "        inf_js = call_inference(prompt)\n",
    "        conf = inf_js.get('confidence', 0.0)\n",
    "        justification = inf_js.get('justification', \"\")\n",
    "        interaction = inf_js.get('interaction', \"None\")\n",
    "    #    \n",
    "        if use_review:\n",
    "            review_decision = call_review(inf_js)\n",
    "            if review_decision and review_decision != 'None':\n",
    "                interaction = review_decision\n",
    "    #   \n",
    "        return {\n",
    "            'source_type': row['source_type'],\n",
    "            'target_type': row['target_type'],\n",
    "            'ligand': row['ligand'],\n",
    "            'receptor': row['receptor'],\n",
    "            'count': row['count'],\n",
    "            'avg_source_norm': row['avg_source_norm'],\n",
    "            'avg_target_norm': row['avg_target_norm'],\n",
    "            'avg_cosine_similarity': row['avg_cosine_similarity'],\n",
    "            'interaction': interaction,\n",
    "            'confidence': conf,\n",
    "            'justification': justification,\n",
    "        }\n",
    "    #\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(executor.map(worker, [row for _, row in agg_df.iterrows()]))\n",
    "    #\n",
    "    results_df = pd.DataFrame(results)\n",
    "    logger.info(f\"Completed inference on all candidates.\")\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Example marker dictionary for prompt building\n",
    "# -----------------------------------------------------------------------------\n",
    "# Run differential expression test per cell type (groups by 'Cell_class')\n",
    "sc.tl.rank_genes_groups(adata, groupby='Cell_class', method='t-test', n_genes=20)\n",
    "\n",
    "# Extract top 10 marker genes per cell type into a dict\n",
    "marker_dict = {}\n",
    "groups = adata.obs['Cell_class'].unique()\n",
    "\n",
    "for group in groups:\n",
    "    # The names stored may be numeric strings (indices) - convert to int indices\n",
    "    indices_str = adata.uns['rank_genes_groups']['names'][group][:10]\n",
    "    # Convert string indices to integers\n",
    "    indices = indices_str.astype(int)\n",
    "    # Map indices to actual gene names from var_names\n",
    "    top_genes = adata.var_names[indices]\n",
    "    marker_dict[group] = list(top_genes)\n",
    "\n",
    "# Inspect marker_dict\n",
    "for cell_type, markers in marker_dict.items():\n",
    "    print(f\"{cell_type}: {markers}\")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Main execution example\n",
    "# -----------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    logger.info(\"Starting evaluation without review step...\")\n",
    "    num_classes = adata.obs['Cell_class'].nunique()\n",
    "    res_single = evaluate_cci_pipeline_with_norms(n_samples=100 * num_classes, use_review=False, max_workers=8)\n",
    "    res_single.to_csv(\"/../cci_inference_results_no_review_0_withDistance.csv\", index=False) \n",
    "\n",
    "    res_single_filtered = res_single[res_single[\"confidence\"] > 0.7].copy()\n",
    "    res_single.to_csv(\"/../cci_inference_results_no_review_0_withDistance_0.7.csv\", index=False)\n",
    "\n",
    "\n",
    "    logger.info(\"Starting evaluation with review step...\")\n",
    "    res_dual = evaluate_cci_pipeline_with_norms(n_samples=100 * num_classes, use_review=True, max_workers=8)\n",
    "    res_dual.to_csv(\"cci_inference_results_with_review.csv\", index=False)\n",
    "\n",
    "    res_dual.to_csv(\"/../cci_inference_results_no_review_0_withDistance_2A.csv\", index=False) \n",
    "    res_dual_filtered = res_dual[res_dual[\"confidence\"] > 0.7].copy()\n",
    "    res_dual_filtered.to_csv(\"/../cci_inference_results_no_review_0_withDistance_2A_0.7.csv\", index=False)\n",
    "\n",
    "    logger.info(\"Evaluation completed.\")\n",
    "\n",
    "\n",
    "# The binning shows a monotonic trend: more occurrences in spatial neighborhoods generally correspond to higher confidence\n",
    "res_single_filtered['count_bin'] = pd.cut(res_single_filtered['count'], bins=[0,1,2,5,10,20,50,100, np.inf])\n",
    "bin_means = res_single_filtered.groupby('count_bin')['confidence'].mean()\n",
    "print(bin_means)\n",
    "\n",
    "correlation = res_single_filtered['avg_source_norm'].corr(res_single_filtered['avg_target_norm'])\n",
    "print(f\"Correlation between avg_source_norm and avg_target_norm: {correlation}\")\n",
    "\n",
    "correlation = res_single_filtered['count'].corr(res_single_filtered['confidence'])\n",
    "print(f\"Correlation between count and confidence: {correlation}\")\n",
    "\n",
    "correlation = res_single_filtered['avg_cosine_similarity'].corr(res_single_filtered['confidence'])\n",
    "print(f\"Correlation between avg_cosine_similarity and confidence: {correlation}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Evaluation cell2cell\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# Contingency matrix\n",
    "# -----------------\n",
    "\n",
    "# Load gold standard\n",
    "gold = pd.read_csv(\n",
    "    \"/../IPF_gold_standard.txt\",\n",
    "    sep=\"\\t\",\n",
    "    encoding=\"utf-16le\"\n",
    ")\n",
    "\n",
    "# Make sure column names match between gold and matched_res\n",
    "gold.columns = [\"source_type\", \"target_type\", \"ligand\", \"receptor\"]\n",
    "\n",
    "# Inner join to find matches\n",
    "matched_in_gold = pd.merge(\n",
    "    res_single_filtered,\n",
    "    gold,\n",
    "    on=[\"source_type\", \"target_type\", \"ligand\", \"receptor\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# Count matches\n",
    "num_matches = len(matched_in_gold)\n",
    "print(f\"Matches with gold standard: {num_matches}\")\n",
    "\n",
    "# Unique source and target cell types in your dataset\n",
    "cell_types = pd.concat([res_single_filtered[\"source_type\"], res_single_filtered[\"target_type\"]]).unique()\n",
    "\n",
    "# All ligands and receptors observed in dataset\n",
    "ligands = pd.concat([res_single_filtered[\"ligand\"], gold[\"ligand\"]]).unique()\n",
    "receptors = pd.concat([res_single_filtered[\"receptor\"], gold[\"receptor\"]]).unique()\n",
    "\n",
    "# Cartesian product to generate all possible pairs\n",
    "allpairs = pd.DataFrame(list(itertools.product(cell_types, cell_types, ligands, receptors)),\n",
    "                        columns=[\"source_type\",\"target_type\",\"ligand\",\"receptor\"])\n",
    "\n",
    "print(\"Total possible pairs:\", len(allpairs))\n",
    "\n",
    "\n",
    "def run_CellPhoneDB_SSP(posi, allpairs, gold):\n",
    "    # Only keep relevant columns\n",
    "    posi = posi[[\"source_type\", \"target_type\", \"ligand\", \"receptor\"]].copy()\n",
    "    \n",
    "    # Create interaction IDs for both directions\n",
    "    posi[\"ID1\"] = posi[\"source_type\"] + \"-\" + posi[\"target_type\"] + \"-\" + posi[\"ligand\"] + \"-\" + posi[\"receptor\"]\n",
    "    posi[\"ID2\"] = posi[\"target_type\"] + \"-\" + posi[\"source_type\"] + \"-\" + posi[\"receptor\"] + \"-\" + posi[\"ligand\"]\n",
    "    \n",
    "    gold = gold[[\"source_type\", \"target_type\", \"ligand\", \"receptor\"]].copy()\n",
    "    gold[\"ID\"] = gold[\"source_type\"] + \"-\" + gold[\"target_type\"] + \"-\" + gold[\"ligand\"] + \"-\" + gold[\"receptor\"]\n",
    "    \n",
    "    # True Positives\n",
    "    TP = ((posi[\"ID1\"].isin(gold[\"ID\"])) | (posi[\"ID2\"].isin(gold[\"ID\"]))).sum()\n",
    "    \n",
    "    # False Positives\n",
    "    FP = len(posi) - TP\n",
    "    \n",
    "    # False Negatives\n",
    "    FN = len(gold) - TP\n",
    "    \n",
    "    # True Negatives\n",
    "    TN = len(allpairs) - (TP + FP + FN)\n",
    "    \n",
    "    # Metrics\n",
    "    acc = (TP + TN) / len(allpairs)\n",
    "    pre = TP / (TP + FP) if (TP + FP) > 0 else np.nan\n",
    "    sst = TP / (TP + FN) if (TP + FN) > 0 else np.nan  # sensitivity\n",
    "    spc = TN / (FP + TN) if (FP + TN) > 0 else np.nan  # specificity\n",
    "    f1 = 2 * TP / (2 * TP + FP + FN) if (2 * TP + FP + FN) > 0 else np.nan\n",
    "    mcc = ((TP * TN - FP * FN) / np.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))) if all(x>0 for x in [TP+FP, TP+FN, TN+FP, TN+FN]) else np.nan\n",
    "    npv = TN / (TN + FN) if (TN + FN) > 0 else np.nan\n",
    "    \n",
    "    SSP = {\n",
    "        \"Precision\": pre,\n",
    "        \"Sensitivity\": sst,\n",
    "        \"Specificity\": spc,\n",
    "        \"F1score\": f1,\n",
    "        \"MCC\": mcc,\n",
    "        \"NPV\": npv,\n",
    "        \"Accuracy\": acc\n",
    "    }\n",
    "    \n",
    "    return SSP\n",
    "\n",
    "\n",
    "# Assuming res_single_filtered is your LLM-filtered results (confidence > 0.7)\n",
    "# allpairs can be all possible source-target-ligand-receptor combinations\n",
    "metrics = run_CellPhoneDB_SSP(res_single_filtered, allpairs, gold)\n",
    "metrics = {k: round(float(v), 7) for k, v in metrics.items()}\n",
    "print(metrics)\n",
    "\n",
    "\n",
    "# Per-cell gene expression distribution\n",
    "# -----------------\n",
    "\n",
    "cell_idx = 0  # or any cell index you want to inspect\n",
    "cell_expr = adata.X[cell_idx]\n",
    "\n",
    "# If sparse matrix, convert to dense\n",
    "if hasattr(cell_expr, \"toarray\"):\n",
    "    cell_expr = cell_expr.toarray().flatten()\n",
    "elif hasattr(cell_expr, \"A1\"):\n",
    "    cell_expr = cell_expr.A1\n",
    "else:\n",
    "    cell_expr = np.asarray(cell_expr).flatten()\n",
    "\n",
    "plt.hist(cell_expr, bins=100, log=True)\n",
    "plt.xlabel(\"Expression value\")\n",
    "plt.ylabel(\"Frequency (log scale)\")\n",
    "plt.title(f\"Expression distribution for cell index {cell_idx}\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate fraction of genes expressed above threshold candidates\n",
    "for thresh in [0.01, 0.05, 0.1, 0.2, 0.5, 1.0, 2., 3.0, 4.0, 5.0, 7.5, 10]:\n",
    "    fraction = np.mean(cell_expr > thresh)\n",
    "    print(f\"Fraction of genes with expr > {thresh}: {fraction:.4f}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Frozen priors\n",
    "# Train RF on LLM outputs, test on original LLM rows,\n",
    "# evaluate on 10 new LR input sets,\n",
    "# evaluate on 10 biologically-derived FALSE LR sets.\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0. ASSUMPTIONS\n",
    "#    adata, knn_adjacency, spatial_embeddings, \n",
    "#    aggregate_interactions_with_norms()\n",
    "#    already exist in memory.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. LOAD LLM OUTPUT (TRAINING DATA)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "df_llm = pd.read_csv(\n",
    "    \"/../cci_inference_results_no_review_0_withDistance_0.7.csv\"\n",
    ")\n",
    "\n",
    "df_llm[\"label\"] = df_llm[\"interaction\"].isin([\"True\", \"Plausible\"]).astype(int)\n",
    "\n",
    "categorical = [\"source_type\", \"target_type\", \"ligand\", \"receptor\"]\n",
    "numeric = [\"count\", \"avg_source_norm\", \"avg_target_norm\", \"avg_cosine_similarity\"]\n",
    "\n",
    "print(\"Training data:\", df_llm.shape)\n",
    "print(\"LLM % positive:\", df_llm[\"label\"].mean() * 100)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1b. LOAD CURATED LR DATABASE\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "lr_pairs = pd.read_csv(\n",
    "    \"/../combined_ligand_receptor_pairs.csv\"\n",
    ")\n",
    "\n",
    "known_lr_set = set(zip(lr_pairs[\"ligand_gene\"], lr_pairs[\"receptor_gene\"]))\n",
    "\n",
    "all_ligands = sorted(df_llm[\"ligand\"].unique())\n",
    "all_receptors = sorted(df_llm[\"receptor\"].unique())\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1c. PRECOMPUTE EXPRESSION PRESENCE PER CELL TYPE\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "expr_threshold = 1.0\n",
    "cell_types = adata.obs[\"Cell_class\"].unique()\n",
    "\n",
    "gene_to_idx = {g: i for i, g in enumerate(adata.var_names)}\n",
    "genes_of_interest = [g for g in (set(all_ligands) | set(all_receptors)) if g in gene_to_idx]\n",
    "\n",
    "expr_present = {}\n",
    "\n",
    "print(\"\\nPrecomputing expression presence across cell types...\")\n",
    "for ct in cell_types:\n",
    "    mask = (adata.obs[\"Cell_class\"] == ct).values\n",
    "    X_sub = adata.X[mask, :]\n",
    "#\n",
    "    for g in genes_of_interest:\n",
    "        gi = gene_to_idx[g]\n",
    "        col = X_sub[:, gi]\n",
    "#\n",
    "        if hasattr(col, \"A1\"):\n",
    "            vals = col.A1\n",
    "        elif hasattr(col, \"toarray\"):\n",
    "            vals = col.toarray().ravel()\n",
    "        else:\n",
    "            vals = np.asarray(col).ravel()\n",
    "#\n",
    "        expr_present[(ct, g)] = (vals > expr_threshold).any()\n",
    "\n",
    "print(\"✓ Expression matrix ready.\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. ONE-HOT ENCODER (TRAIN ONLY)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "enc = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "enc.fit(df_llm[categorical])\n",
    "\n",
    "X_train_full = np.hstack([\n",
    "    df_llm[numeric].values,\n",
    "    enc.transform(df_llm[categorical])\n",
    "])\n",
    "\n",
    "y_train = df_llm[\"label\"]\n",
    "\n",
    "print(\"Training feature size:\", X_train_full.shape)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. TRAIN RANDOM FOREST\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf.fit(X_train_full, y_train)\n",
    "\n",
    "print(\"\\nRF performance on training (LLM) data:\")\n",
    "print(classification_report(y_train, rf.predict(X_train_full)))\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. TEST ON ORIGINAL LLM ROWS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "X_test_full = np.hstack([\n",
    "    df_llm[numeric].values,\n",
    "    enc.transform(df_llm[categorical])\n",
    "])\n",
    "\n",
    "df_llm[\"rf_pred\"] = rf.predict(X_test_full)\n",
    "df_llm[\"rf_prob\"] = rf.predict_proba(X_test_full)[:, 1]\n",
    "\n",
    "print(\"\\nRF % positive on LLM rows:\", df_llm[\"rf_pred\"].mean() * 100)\n",
    "df_llm.to_csv(\"/../rf_on_llm_rows.csv\", index=False)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. GENERATE 10 NEW LR DATASETS (REAL POSITIVES)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def generate_lr_dataset(seed):\n",
    "    np.random.seed(seed)\n",
    "    return aggregate_interactions_with_norms(\n",
    "        adata, knn_adjacency, spatial_embeddings,\n",
    "        n_cells=100 * adata.obs[\"Cell_class\"].nunique(),\n",
    "        expr_threshold=1\n",
    "    )\n",
    "\n",
    "print(\"\\n=== GENERATING 10 NEW POSITIVE-LIKE DATASETS ===\\n\")\n",
    "\n",
    "new_datasets = []\n",
    "pos_summary = []\n",
    "\n",
    "for i in range(10):\n",
    "    seed = 1000 + i\n",
    "    agg_new = generate_lr_dataset(seed)\n",
    "#\n",
    "    X_full = np.hstack([\n",
    "        agg_new[numeric].values,\n",
    "        enc.transform(agg_new[categorical])\n",
    "    ])\n",
    "#\n",
    "    agg_new[\"rf_pred\"] = rf.predict(X_full)\n",
    "    agg_new[\"rf_prob\"] = rf.predict_proba(X_full)[:, 1]\n",
    "#\n",
    "    pos_rate = agg_new[\"rf_pred\"].mean() * 100\n",
    "    print(f\"Dataset {i+1}: {pos_rate:.2f}% positives\")\n",
    "#\n",
    "    pos_summary.append(pos_rate)\n",
    "    new_datasets.append(agg_new)\n",
    "\n",
    "pd.Series(pos_summary).to_csv(\"/../rf_newdatasets_posrate.csv\", index=False)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. BIOLOGICALLY-INFORMED FALSE EXAMPLES FROM ADATA\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def generate_false_examples(\n",
    "    agg_df, ligand_list, receptor_list,\n",
    "    expr_present, known_lr_set,\n",
    "    n_examples=10, seed=0\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    false_rows = []\n",
    "#\n",
    "    src_types = agg_df[\"source_type\"].unique()\n",
    "    tgt_types = agg_df[\"target_type\"].unique()\n",
    "#\n",
    "    for _ in range(n_examples):\n",
    "        s = rng.choice(src_types)\n",
    "        t = rng.choice(tgt_types)\n",
    "#\n",
    "        lig_candidates = [L for L in ligand_list if not expr_present.get((s, L), True)]\n",
    "        rec_candidates = [R for R in receptor_list if not expr_present.get((t, R), True)]\n",
    "#\n",
    "        if len(lig_candidates) == 0 or len(rec_candidates) == 0:\n",
    "            continue\n",
    "#\n",
    "        # ensure LR not in curated DB\n",
    "        for _ in range(30):\n",
    "            L = rng.choice(lig_candidates)\n",
    "            R = rng.choice(rec_candidates)\n",
    "            if (L, R) not in known_lr_set:\n",
    "                break\n",
    "#\n",
    "        false_rows.append({\n",
    "            \"source_type\": s,\n",
    "            \"target_type\": t,\n",
    "            \"ligand\": L,\n",
    "            \"receptor\": R,\n",
    "            \"count\": 0,\n",
    "            \"avg_source_norm\": rng.uniform(0, 3),\n",
    "            \"avg_target_norm\": rng.uniform(0, 3),\n",
    "            \"avg_cosine_similarity\": rng.uniform(-1, -0.1),\n",
    "            \"true_label\": 0\n",
    "        })\n",
    "#\n",
    "    return pd.DataFrame(false_rows)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7) EVALUATE RF ON STRONG FALSE EXAMPLES\n",
    "# ============================================================\n",
    "\n",
    "false_results = []   # STORE NEGATIVE RESULTS FOR METADATA\n",
    "\n",
    "print(\"\\n=== RF performance on BIOLOGICALLY-INFORMED FALSE examples ===\\n\")\n",
    "\n",
    "for i, df_new in enumerate(new_datasets):\n",
    "    print(f\"\\n--- Dataset {i+1} ---\")\n",
    "#\n",
    "    df_false = generate_false_examples_strong(\n",
    "        df_new,\n",
    "        all_ligands,\n",
    "        all_receptors,\n",
    "        expr_present,\n",
    "        known_lr_set,\n",
    "        n_examples=10,\n",
    "        seed=2000 + i\n",
    "    )\n",
    "#\n",
    "    if df_false.empty:\n",
    "        print(f\"Dataset {i+1}: no strong false LR generated.\")\n",
    "        false_results.append(pd.DataFrame())   # keep placeholder\n",
    "        continue\n",
    "#\n",
    "    # Prepare features\n",
    "    X_test_cat = df_false[categorical]\n",
    "    X_test_num = df_false[numeric]\n",
    "#\n",
    "    X_test_full = np.hstack([\n",
    "        X_test_num.values,\n",
    "        enc.transform(X_test_cat)\n",
    "    ])\n",
    "#\n",
    "    df_false[\"rf_pred\"] = rf.predict(X_test_full)\n",
    "    df_false[\"rf_prob\"] = rf.predict_proba(X_test_full)[:, 1]\n",
    "#\n",
    "    n_fp = int(df_false[\"rf_pred\"].sum())\n",
    "    total = len(df_false)\n",
    "#\n",
    "    print(f\"Dataset {i+1}: FALSE predicted TRUE = {n_fp}/{total}\")\n",
    "#\n",
    "    # store negatives\n",
    "    false_results.append(df_false)\n",
    "#\n",
    "    # save negative examples file\n",
    "    df_false.to_csv(f\"rf_negatives_newset_{i+1}.csv\", index=False)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# GLOBAL METADATA\n",
    "# ============================================================\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1) LLM TRAINING DISTRIBUTION\n",
    "# ------------------------------------------------------------\n",
    "llm_true_pct = df_llm[\"label\"].mean() * 100\n",
    "llm_false_pct = 100 - llm_true_pct\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2) RF 75/25 SPLIT PERFORMANCE\n",
    "# ------------------------------------------------------------\n",
    "X = X_train_full\n",
    "y = y_train\n",
    "\n",
    "X_train, X_test, y_train_split, y_test_split = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "rf_split = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_split.fit(X_train, y_train_split)\n",
    "\n",
    "train_acc = rf_split.score(X_train, y_train_split) * 100\n",
    "test_acc = rf_split.score(X_test, y_test_split) * 100\n",
    "\n",
    "train_true_pct = y_train_split.mean() * 100\n",
    "test_true_pct = y_test_split.mean() * 100\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3) GOLD-STANDARD BIOLOGY DISTRIBUTION\n",
    "# ------------------------------------------------------------\n",
    "gold_labels = df_llm.apply(\n",
    "    lambda row: int((row[\"ligand\"], row[\"receptor\"]) in known_lr_set),\n",
    "    axis=1\n",
    ")\n",
    "gold_true_pct = gold_labels.mean() * 100\n",
    "gold_false_pct = 100 - gold_true_pct\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SAVE GLOBAL METADATA TABLE\n",
    "# ------------------------------------------------------------\n",
    "global_metadata = pd.DataFrame([{\n",
    "    \"llm_true_pct\": llm_true_pct,\n",
    "    \"llm_false_pct\": llm_false_pct,\n",
    "    \"rf_train_accuracy_pct\": train_acc,\n",
    "    \"rf_test_accuracy_pct\": test_acc,\n",
    "    \"train_true_pct\": train_true_pct,\n",
    "    \"test_true_pct\": test_true_pct,\n",
    "    \"gold_true_pct\": gold_true_pct,\n",
    "    \"gold_false_pct\": gold_false_pct\n",
    "}])\n",
    "\n",
    "global_metadata.to_csv(\"/../rf_global_metadata.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== SAVED GLOBAL METADATA → rf_global_metadata.csv ===\")\n",
    "print(global_metadata)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# DATASET-LEVEL METADATA\n",
    "# ============================================================\n",
    "\n",
    "dataset_records = []\n",
    "\n",
    "for i in range(10):\n",
    "    dataset_id = i + 1\n",
    "    seed = 1000 + i\n",
    "#\n",
    "    # --- positive dataset ---\n",
    "    agg_new = new_datasets[i]\n",
    "    n_rows = len(agg_new)\n",
    "    pos_rate = agg_new[\"rf_pred\"].mean() * 100\n",
    "#\n",
    "    # --- negative dataset ---\n",
    "    df_false = false_results[i]\n",
    "    n_neg = len(df_false)\n",
    "    neg_fp = int(df_false[\"rf_pred\"].sum()) if n_neg > 0 else 0\n",
    "    neg_fp_rate = (neg_fp / n_neg * 100) if n_neg > 0 else np.nan\n",
    "#\n",
    "    dataset_records.append({\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"seed\": seed,\n",
    "        \"n_rows\": n_rows,\n",
    "        \"positive_rate_percent\": pos_rate,\n",
    "        \"n_negatives_generated\": n_neg,\n",
    "        \"negatives_pred_true\": neg_fp,\n",
    "        \"negative_false_positive_rate\": neg_fp_rate\n",
    "    })\n",
    "\n",
    "dataset_metadata_df = pd.DataFrame(dataset_records)\n",
    "dataset_metadata_df.to_csv(\"/../rf_dataset_metadata.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== SAVED DATASET-LEVEL METADATA → rf_dataset_metadata.csv ===\")\n",
    "print(dataset_metadata_df)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
